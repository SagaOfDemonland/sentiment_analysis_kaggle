{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZnTAU2Yxem9"
      },
      "source": [
        "# Fine-tuning the BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "-XHifb8HxnLS"
      },
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsTOg-jVxem_"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "sCqU0gA5xem_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers import TrainerCallback\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from datasets import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "# Add the src directory to the system path\n",
        "sys.path.append(os.path.abspath('../src'))\n",
        "from config import MODEL_NAME, RANDOM_SEED, BATCH_SIZE, LEARNING_RATE, EPOCHS, MODEL_SAVE_DIR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3Kzzz0ExenA"
      },
      "source": [
        "## Load the Processed Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "0_EiPRuRxenB"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('../data/processed/processed_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "W0NTwUE1xenB"
      },
      "outputs": [],
      "source": [
        "# Shuffle and split the dataset with fixed seed 42\n",
        "df = df.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
        "\n",
        "# Perform stratified split to maintain balance in training and validation sets\n",
        "train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df['Liked'], random_state=RANDOM_SEED)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['Liked'], random_state=RANDOM_SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "S889xzym2AMF"
      },
      "outputs": [],
      "source": [
        "train_df['labels'] = train_df['Liked']\n",
        "val_df['labels'] = val_df['Liked']\n",
        "test_df['labels'] = test_df['Liked']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6zgKhS2xenB"
      },
      "source": [
        "## Construct HF Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "TLy7GM8KxenB"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lj5mtbCxxenC"
      },
      "outputs": [],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AAtU8W2xenC"
      },
      "outputs": [],
      "source": [
        "test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKB_FAD-xenC"
      },
      "outputs": [],
      "source": [
        "val_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HVlvUEwxenC"
      },
      "source": [
        "## Tokenizing the `Review` from the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hH_6ApotxenC"
      },
      "outputs": [],
      "source": [
        "# Load tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "FUHB7NLixenD"
      },
      "outputs": [],
      "source": [
        "# Tokenize helper function\n",
        "def tokenize_function(examples):\n",
        "    # Tokenize the 'Review' column\n",
        "    tokenized = tokenizer(examples['Review'], padding='max_length', truncation=True)\n",
        "    # Include 'labels' in the tokenized output if available\n",
        "    tokenized['labels'] = examples['labels']\n",
        "    return tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IimihRnxenD"
      },
      "outputs": [],
      "source": [
        "# Tokenize datasets\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "4vyylSl7xenD"
      },
      "outputs": [],
      "source": [
        "# Set format for PyTorch\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TcsBjRYxenD"
      },
      "source": [
        "## Training the `bert-base-uncased` Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swbW5HIRxenD"
      },
      "source": [
        "### Training Arguments with Version Control for Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "8GKAfYM3xenD"
      },
      "outputs": [],
      "source": [
        "version = 1\n",
        "model_path = os.path.join(MODEL_SAVE_DIR, f\"{MODEL_NAME}_v{version}\")\n",
        "while os.path.exists(model_path):\n",
        "    version += 1\n",
        "    model_path = os.path.join(MODEL_SAVE_DIR, f\"{MODEL_NAME}_v{version}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV7Unk3uxenE"
      },
      "source": [
        "By default, Hugging Face's Trainer uses the AdamW optimizer and cross-entropy loss for classification tasks. Here, we just use default settings, but we could take customized functions to override later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyK17IMJxenE"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=model_path,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=f\"{model_path}/logs\",\n",
        "    seed=RANDOM_SEED,\n",
        "    load_best_model_at_end=True,\n",
        "    # gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    # lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
        "    no_cuda=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "BEUjDQw9-IZM"
      },
      "outputs": [],
      "source": [
        "class LossLoggerCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.train_losses = []\n",
        "        self.eval_losses = []\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if logs is not None:\n",
        "            # Save train and eval losses if they exist in logs\n",
        "            if \"loss\" in logs:\n",
        "                self.train_losses.append(logs[\"loss\"])\n",
        "            if \"eval_loss\" in logs:\n",
        "                self.eval_losses.append(logs[\"eval_loss\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "0nlp9hUy-XgH"
      },
      "outputs": [],
      "source": [
        "def save_losses_and_plot(train_losses, eval_losses, output_dir=\".\", filename=\"losses.txt\"):\n",
        "    # Save losses to a text file\n",
        "    with open(os.path.join(output_dir, filename), \"w\") as f:\n",
        "        f.write(\"Epoch\\tTraining Loss\\tValidation Loss\\n\")\n",
        "        for epoch, (train_loss, eval_loss) in enumerate(zip(train_losses, eval_losses), start=1):\n",
        "            f.write(f\"{epoch}\\t{train_loss}\\t{eval_loss}\\n\")\n",
        "\n",
        "    # Plot the losses\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\", marker='o')\n",
        "    plt.plot(range(1, len(eval_losses) + 1), eval_losses, label=\"Validation Loss\", marker='o')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.savefig(os.path.join(output_dir, \"loss_plot.png\"))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "dK80hxvL-niD"
      },
      "outputs": [],
      "source": [
        "loss_logger = LossLoggerCallback()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yed_Qf96xenE"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    callbacks=[loss_logger]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZyCUtk3xenE"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the final model and tokenizer\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n",
        "print(f\"Model saved to {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qf8jfKjn_ESX"
      },
      "outputs": [],
      "source": [
        "save_losses_and_plot(loss_logger.train_losses, loss_logger.eval_losses, output_dir=model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UroIBEN_5Omj"
      },
      "source": [
        "Simly load the model checkpoints for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "I4T9WJ4cxenE"
      },
      "outputs": [],
      "source": [
        "# Load the fine-tuned model and tokenizer\n",
        "model_path = '../data/models/bert-base-uncased_v1'\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aixwRgIS5Vzw"
      },
      "outputs": [],
      "source": [
        "# Initialize a Trainer instance (only for prediction)\n",
        "trainer = Trainer(model=model)\n",
        "\n",
        "# Perform prediction on the test dataset\n",
        "predictions = trainer.predict(test_dataset)\n",
        "predicted_labels = torch.argmax(torch.tensor(predictions.predictions), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8eHUnAS5tx3"
      },
      "outputs": [],
      "source": [
        "# Convert predictions to a list and add as a new column to test_df\n",
        "test_df['Predicted'] = predicted_labels.numpy()  # Convert tensor to numpy array, then add to DataFrame\n",
        "\n",
        "# Display the first few rows to compare actual vs. predicted\n",
        "print(test_df[['Review', 'Liked', 'Predicted']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLnYUDnQ8aWn"
      },
      "outputs": [],
      "source": [
        "# Calculate the confusion matrix\n",
        "conf_matrix = confusion_matrix(test_df['Liked'], test_df['Predicted'])\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(test_df['Liked'], test_df['Predicted'])\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\\nAccuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQWOxt__8qsf"
      },
      "outputs": [],
      "source": [
        "incorrect_predictions = test_df[test_df['Liked'] != test_df['Predicted']]\n",
        "\n",
        "# Display the incorrect predictions\n",
        "print(incorrect_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFnKwqfW82AO"
      },
      "outputs": [],
      "source": [
        "len(incorrect_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you don't have cuda installed in your machine, use the following method to call the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "mbHE3T0A3WXJ"
      },
      "outputs": [],
      "source": [
        "# Ensure the model is on the CPU\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_path,\n",
        "    # evaluation_strategy=\"epoch\",\n",
        "    # save_strategy=\"epoch\",\n",
        "    # logging_strategy=\"epoch\",\n",
        "    # learning_rate=LEARNING_RATE,\n",
        "    # per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    # num_train_epochs=EPOCHS,\n",
        "    # weight_decay=0.01,\n",
        "    # logging_dir=f\"{model_path}/logs\",\n",
        "    # seed=RANDOM_SEED,\n",
        "    # load_best_model_at_end=True,\n",
        "    # gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    # lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
        "    no_cuda=True\n",
        ")\n",
        "\n",
        "# Initialize the Trainer instance with CPU-only settings\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcidv1QU_mH2"
      },
      "outputs": [],
      "source": [
        "# Perform prediction on the test dataset\n",
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "# Extract the predicted labels\n",
        "predicted_labels = torch.argmax(torch.tensor(predictions.predictions), axis=1)\n",
        "\n",
        "# Convert predictions to a list and add as a new column to test_df\n",
        "test_df['Predicted'] = predicted_labels.numpy()  # Convert tensor to numpy array\n",
        "\n",
        "# Display the first few rows to compare actual vs. predicted\n",
        "print(test_df[['Review', 'Liked', 'Predicted']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate the confusion matrix\n",
        "conf_matrix = confusion_matrix(test_df['Liked'], test_df['Predicted'])\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(test_df['Liked'], test_df['Predicted'])\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\\nAccuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "incorrect_predictions = test_df[test_df['Liked'] != test_df['Predicted']]\n",
        "\n",
        "# Display the incorrect predictions\n",
        "print(incorrect_predictions)\n",
        "print(len(incorrect_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
